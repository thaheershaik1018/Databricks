{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3953c335-1469-4cb4-b412-7e25dbf27694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Modeling <br>\n",
    "\n",
    "Data modeling is the process of creating a visual representation of a system's data and its relationships. It defines how data is stored, connected, processed, and retrieved in databases or data warehouses.\n",
    "<br>\n",
    "  OR  <br>\n",
    "its a process of creating the blueprint how data is stored,connected and retrived from the systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dae12895-9726-4481-a0f6-e24938d48067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDCCA Types of Data Models\n",
    "\n",
    "Data modeling is the process of visually representing how data is stored, connected, and accessed. There are three primary types of data models:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. \uD83E\uDDE0 Conceptual Data Model\n",
    "- **Purpose**: High-level overview of business data.\n",
    "- **Audience**: Business stakeholders and analysts.\n",
    "- **Focus**: What data is important (not how it's stored).\n",
    "- **Example**: \n",
    "  - Entities: Customer, Product, Order\n",
    "  - No attributes or data types specified.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. \uD83D\uDCD8 Logical Data Model\n",
    "- **Purpose**: More detailed view including relationships and attributes.\n",
    "- **Audience**: Data architects, analysts.\n",
    "- **Focus**: Defines data structure regardless of physical implementation.\n",
    "- **Includes**:\n",
    "  - Entities\n",
    "  - Attributes (with data types)\n",
    "  - Primary and foreign keys\n",
    "  - Relationships (1:1, 1:N, M:N)\n",
    "- **Example**: An ER diagram with tables like `Customer`, `Order`, `Product`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. \uD83D\uDCBD Physical Data Model\n",
    "- **Purpose**: Actual implementation on a specific database or system.\n",
    "- **Audience**: DBAs, developers.\n",
    "- **Focus**: Performance, indexing, storage format.\n",
    "- **Includes**:\n",
    "  - Table names, column names\n",
    "  - Data types, indexes, partitions\n",
    "  - Constraints (PK, FK, NOT NULL, etc.)\n",
    "- **Example**: SQL table definitions in Snowflake, SQL Server, Databricks, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDCCC Comparison Table\n",
    "\n",
    "| Feature             | Conceptual Model | Logical Model       | Physical Model         |\n",
    "|---------------------|------------------|----------------------|-------------------------|\n",
    "| Purpose             | Business view     | Logical structure    | Implementation detail   |\n",
    "| Includes attributes | ❌                | ✅                   | ✅                      |\n",
    "| Includes data types | ❌                | ✅ (abstract)        | ✅ (actual)             |\n",
    "| Tied to DBMS        | ❌                | ❌                   | ✅                      |\n",
    "| Audience            | Business          | Data Architects      | Engineers / DBAs        |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- Use **conceptual** to define the *what*.\n",
    "- Use **logical** to define the *how* (abstractly).\n",
    "- Use **physical** to define the *exact implementation*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "719599de-95e8-45ff-aafc-d21c97077c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDD04 OLTP vs OLAP\n",
    "\n",
    "Understanding the difference between **OLTP (Online Transaction Processing)** and **OLAP (Online Analytical Processing)** is key in data engineering and data warehousing.\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDD39 What is OLTP?\n",
    "\n",
    "**OLTP (Online Transaction Processing)** systems are designed to handle **day-to-day transactional data**.\n",
    "\n",
    "- \uD83D\uDCE5 **Use Case**: Inserting, updating, and deleting records in real-time.\n",
    "- \uD83C\uDFE2 **Example**: Banking systems, e-commerce order systems, ticket booking systems.\n",
    "\n",
    "### ✅ Key Features of OLTP:\n",
    "- Handles **high volume of short transactions**\n",
    "- Data is **highly normalized**\n",
    "- Focuses on **data integrity and speed**\n",
    "- Supports **CRUD operations** (Create, Read, Update, Delete)\n",
    "- **Real-time access** to operational data\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDD38 What is OLAP?\n",
    "\n",
    "**OLAP (Online Analytical Processing)** systems are used for **data analysis and reporting**.\n",
    "\n",
    "- \uD83D\uDCCA **Use Case**: Running complex queries, dashboards, historical data analysis.\n",
    "- \uD83C\uDFE2 **Example**: Business intelligence tools, data warehouses, Power BI dashboards.\n",
    "\n",
    "### ✅ Key Features of OLAP:\n",
    "- Handles **complex queries over large datasets**\n",
    "- Data is often **denormalized** (star or snowflake schema)\n",
    "- Supports **aggregations, drill-downs, slicing/dicing**\n",
    "- Used for **decision making**\n",
    "- Data is typically **read-heavy** and not updated frequently\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83C\uDD9A OLTP vs OLAP: Comparison Table\n",
    "\n",
    "| Feature                 | OLTP                                | OLAP                              |\n",
    "|-------------------------|-------------------------------------|-----------------------------------|\n",
    "| Stands For              | Online Transaction Processing       | Online Analytical Processing      |\n",
    "| Purpose                 | Day-to-day operations               | Analysis and decision support     |\n",
    "| Data Structure          | Normalized                          | Denormalized (Star/Snowflake)     |\n",
    "| Operations              | Read & Write (Insert/Update/Delete) | Read-heavy (complex queries)      |\n",
    "| Query Type              | Simple, fast transactions           | Complex analytical queries        |\n",
    "| Data Volume             | Small transactions, high volume     | Large datasets                    |\n",
    "| Users                   | End users, clerks, front-line staff | Analysts, managers, data scientists |\n",
    "| Examples                | ATM systems, online stores          | Sales dashboard, marketing analysis |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- Use **OLTP** systems for **transactional** workloads.\n",
    "- Use **OLAP** systems for **analytical** workloads.\n",
    "- In a modern data architecture, **data flows from OLTP → OLAP** through ETL/ELT pipelines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3568b855-f9bb-4c78-b3a3-38d77a4e8d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDCBE Persistent vs ⚡ Transient Data\n",
    "\n",
    "Understanding the difference between **persistent** and **transient** data is important when designing data pipelines, managing storage, and optimizing performance.\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDCBE Persistent Data\n",
    "\n",
    "- **Stored permanently** until explicitly deleted.\n",
    "- Survives system crashes, restarts, or job failures.\n",
    "- Used for long-term storage, compliance, and analytics.\n",
    "\n",
    "### ✅ Characteristics:\n",
    "- Stored on disk or cloud (e.g., ADLS, S3, HDFS, databases).\n",
    "- Can be queried or retrieved multiple times.\n",
    "- Examples:\n",
    "  - Tables in Delta Lake\n",
    "  - Data stored in SQL or NoSQL databases\n",
    "  - Parquet/CSV files stored in data lakes\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Transient Data\n",
    "\n",
    "- **Temporary** data used during processing or within a session.\n",
    "- Lost when the session ends or the process completes.\n",
    "- Used for intermediate results, testing, or caching.\n",
    "\n",
    "### ⚠️ Characteristics:\n",
    "- Stored in memory or temporary storage.\n",
    "- Not intended for long-term access.\n",
    "- Examples:\n",
    "  - Spark DataFrames (if not saved)\n",
    "  - Temporary SQL views or tables\n",
    "  - Session variables or pipeline staging buffers\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDD0D Comparison Table\n",
    "\n",
    "| Feature             | Persistent Data                     | Transient Data                        |\n",
    "|---------------------|--------------------------------------|----------------------------------------|\n",
    "| Storage Medium      | Disk, cloud, databases               | Memory, temp disk                      |\n",
    "| Lifetime            | Until deleted manually               | Lost after session/job ends            |\n",
    "| Use Case            | Final storage, analytics, reporting  | Intermediate transformations           |\n",
    "| Durability          | High                                 | Low                                    |\n",
    "| Recovery after crash| Possible                             | Not possible                           |\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83E\uDDE0 Summary\n",
    "\n",
    "- Use **persistent storage** when you need durability, compliance, and historical tracking.\n",
    "- Use **transient storage** to optimize performance and reduce storage costs during processing stages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f09b0d4-37d9-4c70-acf9-064caa342929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDD04 Incremental Loading in Data Engineering\n",
    "\n",
    "**Incremental loading** is the process of loading only **new or updated data** from a source system into your data warehouse or data lake, rather than loading the entire dataset every time.\n",
    "\n",
    "This approach is **efficient, faster, and cost-effective**, especially for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDE80 Why Use Incremental Loading?\n",
    "\n",
    "| Benefit             | Description                                      |\n",
    "|---------------------|--------------------------------------------------|\n",
    "| ⏱️ Faster Loads      | Only processes new/changed records               |\n",
    "| \uD83D\uDCB0 Lower Cost        | Reduces compute and storage usage               |\n",
    "| \uD83D\uDCE6 Scalable          | Ideal for large-scale or streaming data systems |\n",
    "| \uD83D\uDD0D Easier Auditing   | Allows tracking changes over time               |\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83E\uDDE9 Types of Incremental Loads\n",
    "\n",
    "### 1. **Append-Only (Insert-only)**\n",
    "- Load only new rows (e.g., new orders, new log entries)\n",
    "- Common with time-based or auto-incremented keys\n",
    "- Simple and efficient\n",
    "\n",
    "### 2. **Upserts (Insert + Update)**\n",
    "- Detect and load both new and **changed rows**\n",
    "- Requires a **unique key** (e.g., order_id, customer_id)\n",
    "- Uses `MERGE`, `UPDATE`, or `DELETE + INSERT` logic\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDD0D How to Identify New/Changed Data\n",
    "\n",
    "- **Timestamps** (e.g., `last_updated`, `order_date`)\n",
    "- **Change Data Capture (CDC)** tools or logs\n",
    "- **Hashing rows** and comparing for changes\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDD27 Example: PySpark with Delta Lake\n",
    "\n",
    "```python\n",
    "# 1. Read new data from source\n",
    "new_data = spark.read.format(\"csv\").load(\"/mnt/source/data.csv\")\n",
    "\n",
    "# 2. Read existing target table\n",
    "target = spark.read.format(\"delta\").load(\"/mnt/delta/silver_table\")\n",
    "\n",
    "# 3. Perform merge (UPSERT)\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/mnt/delta/silver_table\")\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    new_data.alias(\"source\"),\n",
    "    \"target.customer_id = source.customer_id\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Understanding_data_modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}